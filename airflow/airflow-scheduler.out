[[34m2025-03-11T11:55:30.555+0200[0m] {[34mscheduler_job_runner.py:[0m797} INFO[0m - Starting the scheduler[0m
[[34m2025-03-11T11:55:30.555+0200[0m] {[34mscheduler_job_runner.py:[0m804} INFO[0m - Processing each file at most -1 times[0m
[[34m2025-03-11T11:55:30.559+0200[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 138902[0m
[[34m2025-03-11T11:55:30.560+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T11:55:30.562+0200[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2025-03-11T11:55:30.573+0200] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-03-11T12:00:06.140+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:00:06.140+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:00:06.140+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:00:06.141+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task create_base_directory because previous state change time has not been saved[0m
[[34m2025-03-11T12:00:06.142+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2025-03-11T12:00:06.142+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:00:06.144+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:00:06.679+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:00:06.767+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:00:06.796+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:00:06.796+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:00:06.877+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:00:06.954+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:00:04.657498+00:00/task_id=create_base_directory permission to 509
[[34m2025-03-11T12:00:07.193+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:00:07.604+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:00:07.608+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_base_directory, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:00:07.219429+00:00, run_end_date=2025-03-11 10:00:07.289845+00:00, run_duration=0.070416, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=2, pool=default_pool, queue=default, priority_weight=10, operator=BashOperator, queued_dttm=2025-03-11 10:00:06.141152+00:00, queued_by_job_id=1, pid=145609[0m
[[34m2025-03-11T12:00:30.744+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T12:01:35.925+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:01:35.926+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:01:35.926+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:01:35.927+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=2, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2025-03-11T12:01:35.927+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:01:35.929+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:01:36.558+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:01:36.654+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:01:36.691+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:01:36.691+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:01:36.794+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:01:36.908+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-03-11T12:01:37.214+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:01:37.675+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=2, map_index=-1)[0m
[[34m2025-03-11T12:01:37.678+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_base_directory, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:01:37.244845+00:00, run_end_date=2025-03-11 10:01:37.327051+00:00, run_duration=0.082206, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=3, pool=default_pool, queue=default, priority_weight=10, operator=BashOperator, queued_dttm=2025-03-11 10:01:35.926781+00:00, queued_by_job_id=1, pid=147921[0m
[[34m2025-03-11T12:01:57.975+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:01:57.977+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:01:57.977+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:01:57.978+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.upload_beneficiary_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:01:57.978+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:01:57.979+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:01:57.979+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.upload_claims_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:01:57.979+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:01:57.980+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:04:25.703+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.703+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.703+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.703+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.upload_beneficiary_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.703+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.704+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.704+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.upload_claims_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.704+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.704+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.707+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.download_data manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:04:25.709+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.process_data manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:04:25.709+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.upload_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:05:30.818+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2025-03-11T12:06:34.249+0200] {manager.py:543} INFO - DAG medicare_data_extract_s3 is missing and will be deactivated.
[2025-03-11T12:06:34.253+0200] {manager.py:553} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-03-11T12:06:34.262+0200] {manager.py:557} INFO - Deleted DAG medicare_data_extract_s3 in serialized_dag table
[[34m2025-03-11T12:07:06.905+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:07:06.905+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:07:06.906+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:07:06.907+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=3, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2025-03-11T12:07:06.907+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:06.908+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:07.627+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:07:07.747+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:07:07.802+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:07:07.802+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:07:07.897+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:07:08.025+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-03-11T12:07:08.338+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:07:08.820+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=3, map_index=-1)[0m
[[34m2025-03-11T12:07:08.824+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_base_directory, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:07:08.369923+00:00, run_end_date=2025-03-11 10:07:08.455853+00:00, run_duration=0.08593, state=success, executor_state=success, try_number=3, max_tries=2, job_id=4, pool=default_pool, queue=default, priority_weight=10, operator=BashOperator, queued_dttm=2025-03-11 10:07:06.906415+00:00, queued_by_job_id=1, pid=156437[0m
[[34m2025-03-11T12:07:08.985+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:07:08.985+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:07:08.985+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:07:08.986+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task create_data_subdirectories because previous state change time has not been saved[0m
[[34m2025-03-11T12:07:08.986+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_data_subdirectories', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1) to executor with priority 9 and queue default[0m
[[34m2025-03-11T12:07:08.987+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_data_subdirectories', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:08.988+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_data_subdirectories', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:09.679+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:07:09.780+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:07:09.830+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:07:09.830+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:07:09.926+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:07:10.020+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:00:04.657498+00:00/task_id=create_data_subdirectories permission to 509
[[34m2025-03-11T12:07:10.301+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:07:10.950+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_data_subdirectories', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:07:10.955+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_data_subdirectories, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:07:10.334539+00:00, run_end_date=2025-03-11 10:07:10.422996+00:00, run_duration=0.088457, state=success, executor_state=success, try_number=1, max_tries=1, job_id=5, pool=default_pool, queue=default, priority_weight=9, operator=BashOperator, queued_dttm=2025-03-11 10:07:08.985947+00:00, queued_by_job_id=1, pid=156491[0m
[[34m2025-03-11T12:07:11.400+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 3 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:07:11.400+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:07:11.400+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T12:07:11.400+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T12:07:11.400+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:07:11.401+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_beneficiary_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:07:11.401+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_claims_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:07:11.402+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_part_d_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:07:11.402+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_beneficiary_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T12:07:11.402+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_beneficiary_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:11.402+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_claims_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T12:07:11.402+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_claims_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:11.402+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_part_d_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:07:11.402+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_part_d_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:11.404+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_beneficiary_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:12.115+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:07:12.226+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:07:12.272+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:07:12.273+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:07:12.381+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:07:12.496+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:00:04.657498+00:00/task_id=download_beneficiary_data permission to 509
[[34m2025-03-11T12:07:12.846+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:07:17.613+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_claims_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:18.311+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:07:18.398+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:07:18.432+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:07:18.432+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:07:18.512+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:07:18.607+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:00:04.657498+00:00/task_id=download_claims_data permission to 509
[[34m2025-03-11T12:07:18.870+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:09:16.674+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_part_d_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:17.256+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:09:17.331+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:17.362+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:09:17.362+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:17.438+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:09:17.522+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:00:04.657498+00:00/task_id=download_part_d_data permission to 509
[[34m2025-03-11T12:09:17.763+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:09:32.649+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_beneficiary_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:09:32.650+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_claims_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:09:32.650+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_part_d_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:09:32.652+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_beneficiary_data, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:07:12.887957+00:00, run_end_date=2025-03-11 10:07:17.156749+00:00, run_duration=4.268792, state=success, executor_state=success, try_number=1, max_tries=1, job_id=6, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 10:07:11.401165+00:00, queued_by_job_id=1, pid=156567[0m
[[34m2025-03-11T12:09:32.652+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_claims_data, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:07:18.900911+00:00, run_end_date=2025-03-11 10:09:16.325259+00:00, run_duration=117.424348, state=success, executor_state=success, try_number=1, max_tries=1, job_id=7, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 10:07:11.401165+00:00, queued_by_job_id=1, pid=156703[0m
[[34m2025-03-11T12:09:32.652+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_part_d_data, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:09:17.790406+00:00, run_end_date=2025-03-11 10:09:32.301508+00:00, run_duration=14.511102, state=success, executor_state=success, try_number=1, max_tries=1, job_id=8, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:07:11.401165+00:00, queued_by_job_id=1, pid=159693[0m
[[34m2025-03-11T12:09:32.663+0200[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=138902) last sent a heartbeat 141.29 seconds ago! Restarting it[0m
[[34m2025-03-11T12:09:32.667+0200[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 138902. PIDs of all processes in the group: [138902][0m
[[34m2025-03-11T12:09:32.668+0200[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 138902[0m
[[34m2025-03-11T12:09:32.760+0200[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=138902, status='terminated', exitcode=0, started='11:55:30') (138902) terminated with exit code 0[0m
[[34m2025-03-11T12:09:32.764+0200[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 160055[0m
[[34m2025-03-11T12:09:32.770+0200[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2025-03-11T12:09:32.786+0200] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-03-11T12:09:33.088+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 4 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:09:33.088+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:09:33.089+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T12:09:33.089+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T12:09:33.089+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 3/16 running and queued tasks[0m
[[34m2025-03-11T12:09:33.089+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:09:33.090+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task create_base_directory because previous state change time has not been saved[0m
[[34m2025-03-11T12:09:33.090+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_beneficiary_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:09:33.090+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_claims_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task upload_part_d_to_s3 because previous state change time has not been saved[0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_beneficiary_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_beneficiary_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_claims_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_claims_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_part_d_to_s3', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_part_d_to_s3', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:33.093+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:33.599+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:09:33.686+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:33.720+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:09:33.720+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:33.800+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:09:33.882+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:07:28.075748+00:00/task_id=create_base_directory permission to 509
[[34m2025-03-11T12:09:34.124+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:07:28.075748+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:09:34.538+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_beneficiary_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:35.069+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:09:35.151+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:35.181+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:09:35.182+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:35.255+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:09:35.336+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:00:04.657498+00:00/task_id=extract_beneficiary_data permission to 509
[[34m2025-03-11T12:09:35.578+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:09:36.078+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_claims_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:36.628+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:09:36.718+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:36.768+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:09:36.769+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:36.867+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:09:36.948+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:00:04.657498+00:00/task_id=extract_claims_data permission to 509
[[34m2025-03-11T12:09:37.203+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:09:39.415+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_part_d_to_s3', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:39.948+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:09:40.032+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:40.066+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:09:40.066+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:40.147+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:09:40.229+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:00:04.657498+00:00/task_id=upload_part_d_to_s3 permission to 509
[[34m2025-03-11T12:09:40.469+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:09:54.409+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:09:54.409+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_beneficiary_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:09:54.409+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_claims_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:09:54.409+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_part_d_to_s3', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:09:54.415+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_base_directory, run_id=manual__2025-03-11T10:07:28.075748+00:00, map_index=-1, run_start_date=2025-03-11 10:09:34.152882+00:00, run_end_date=2025-03-11 10:09:34.229432+00:00, run_duration=0.07655, state=success, executor_state=success, try_number=1, max_tries=1, job_id=9, pool=default_pool, queue=default, priority_weight=10, operator=BashOperator, queued_dttm=2025-03-11 10:09:33.089833+00:00, queued_by_job_id=1, pid=160100[0m
[[34m2025-03-11T12:09:54.415+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=extract_beneficiary_data, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:09:35.606107+00:00, run_end_date=2025-03-11 10:09:35.774450+00:00, run_duration=0.168343, state=success, executor_state=success, try_number=1, max_tries=1, job_id=10, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:09:33.089833+00:00, queued_by_job_id=1, pid=160137[0m
[[34m2025-03-11T12:09:54.415+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=extract_claims_data, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:09:37.230442+00:00, run_end_date=2025-03-11 10:09:39.105540+00:00, run_duration=1.875098, state=success, executor_state=success, try_number=1, max_tries=1, job_id=11, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:09:33.089833+00:00, queued_by_job_id=1, pid=160178[0m
[[34m2025-03-11T12:09:54.415+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=upload_part_d_to_s3, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:09:40.491658+00:00, run_end_date=2025-03-11 10:09:44.816970+00:00, run_duration=4.325312, state=failed, executor_state=success, try_number=1, max_tries=1, job_id=12, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-03-11 10:09:33.089833+00:00, queued_by_job_id=1, pid=160281[0m
[[34m2025-03-11T12:09:54.570+0200[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun medicare_data_extract_s3 @ 2025-03-11 10:00:04.657498+00:00: manual__2025-03-11T10:00:04.657498+00:00, state:running, queued_at: 2025-03-11 10:00:04.676355+00:00. externally triggered: True> failed[0m
[[34m2025-03-11T12:09:54.571+0200[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=medicare_data_extract_s3, execution_date=2025-03-11 10:00:04.657498+00:00, run_id=manual__2025-03-11T10:00:04.657498+00:00, run_start_date=2025-03-11 10:00:06.107714+00:00, run_end_date=2025-03-11 10:09:54.571456+00:00, run_duration=588.463742, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-03-11 10:00:04.657498+00:00, data_interval_end=2025-03-11 10:00:04.657498+00:00, dag_hash=6476a35378d0bd98617ac0ab63c3bf83[0m
[[34m2025-03-11T12:09:54.578+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>[0m
[[34m2025-03-11T12:09:54.579+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:09:54.579+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>[0m
[[34m2025-03-11T12:09:54.580+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task create_data_subdirectories because previous state change time has not been saved[0m
[[34m2025-03-11T12:09:54.580+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_data_subdirectories', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1) to executor with priority 9 and queue default[0m
[[34m2025-03-11T12:09:54.580+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_data_subdirectories', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:54.582+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_data_subdirectories', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:55.245+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:09:55.335+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:55.371+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:09:55.371+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:55.453+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:09:55.543+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:07:28.075748+00:00/task_id=create_data_subdirectories permission to 509
[[34m2025-03-11T12:09:55.805+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:07:28.075748+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:09:56.253+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_data_subdirectories', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:09:56.256+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_data_subdirectories, run_id=manual__2025-03-11T10:07:28.075748+00:00, map_index=-1, run_start_date=2025-03-11 10:09:55.835814+00:00, run_end_date=2025-03-11 10:09:55.923177+00:00, run_duration=0.087363, state=success, executor_state=success, try_number=1, max_tries=1, job_id=13, pool=default_pool, queue=default, priority_weight=9, operator=BashOperator, queued_dttm=2025-03-11 10:09:54.579527+00:00, queued_by_job_id=1, pid=160702[0m
[[34m2025-03-11T12:09:56.397+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 3 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>[0m
[[34m2025-03-11T12:09:56.397+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:09:56.397+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T12:09:56.397+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T12:09:56.397+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>[0m
[[34m2025-03-11T12:09:56.398+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_beneficiary_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:09:56.398+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_claims_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:09:56.398+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_part_d_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:09:56.398+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_beneficiary_data', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T12:09:56.398+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_beneficiary_data', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:56.398+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_claims_data', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T12:09:56.398+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_claims_data', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:56.398+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_part_d_data', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:09:56.399+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_part_d_data', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:56.400+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_beneficiary_data', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:56.961+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:09:57.041+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:57.073+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:09:57.073+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:57.155+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:09:57.241+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:07:28.075748+00:00/task_id=download_beneficiary_data permission to 509
[[34m2025-03-11T12:09:57.478+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:07:28.075748+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:10:03.206+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_claims_data', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:03.781+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:10:03.871+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:03.905+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:10:03.906+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:03.983+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:10:04.062+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:07:28.075748+00:00/task_id=download_claims_data permission to 509
[[34m2025-03-11T12:10:04.315+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:07:28.075748+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:10:14.807+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_part_d_data', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:15.564+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:10:15.659+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:15.694+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:10:15.694+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:15.771+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:10:15.850+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:07:28.075748+00:00/task_id=download_part_d_data permission to 509
[[34m2025-03-11T12:10:16.087+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:07:28.075748+00:00 [skipped]> on host liran-Zenbook[0m
[[34m2025-03-11T12:10:16.389+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_beneficiary_data', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:10:16.389+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_claims_data', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:10:16.389+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_part_d_data', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:10:16.392+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_beneficiary_data, run_id=manual__2025-03-11T10:07:28.075748+00:00, map_index=-1, run_start_date=2025-03-11 10:09:57.505071+00:00, run_end_date=2025-03-11 10:10:02.893886+00:00, run_duration=5.388815, state=success, executor_state=success, try_number=1, max_tries=1, job_id=14, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 10:09:56.397906+00:00, queued_by_job_id=1, pid=160744[0m
[[34m2025-03-11T12:10:16.392+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_claims_data, run_id=manual__2025-03-11T10:07:28.075748+00:00, map_index=-1, run_start_date=2025-03-11 10:10:04.341711+00:00, run_end_date=2025-03-11 10:10:04.720426+00:00, run_duration=0.378715, state=failed, executor_state=success, try_number=1, max_tries=1, job_id=15, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 10:09:56.397906+00:00, queued_by_job_id=1, pid=160913[0m
[[34m2025-03-11T12:10:16.392+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_part_d_data, run_id=manual__2025-03-11T10:07:28.075748+00:00, map_index=-1, run_start_date=2025-03-11 10:10:04.709523+00:00, run_end_date=2025-03-11 10:10:04.709523+00:00, run_duration=0.0, state=skipped, executor_state=success, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:09:56.397906+00:00, queued_by_job_id=1, pid=None[0m
[[34m2025-03-11T12:10:19.035+0200[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun medicare_data_extract_s3 @ 2025-03-11 10:07:28.075748+00:00: manual__2025-03-11T10:07:28.075748+00:00, state:running, queued_at: 2025-03-11 10:07:28.085286+00:00. externally triggered: True> failed[0m
[[34m2025-03-11T12:10:19.035+0200[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=medicare_data_extract_s3, execution_date=2025-03-11 10:07:28.075748+00:00, run_id=manual__2025-03-11T10:07:28.075748+00:00, run_start_date=2025-03-11 10:09:33.068474+00:00, run_end_date=2025-03-11 10:10:19.035724+00:00, run_duration=45.96725, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-03-11 10:07:28.075748+00:00, data_interval_end=2025-03-11 10:07:28.075748+00:00, dag_hash=6476a35378d0bd98617ac0ab63c3bf83[0m
[[34m2025-03-11T12:10:20.205+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:10:20.205+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:10:20.205+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:10:20.206+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task create_base_directory because previous state change time has not been saved[0m
[[34m2025-03-11T12:10:20.206+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2025-03-11T12:10:20.206+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:20.207+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:20.780+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:10:20.870+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:20.903+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:10:20.903+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:20.984+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:10:21.073+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=create_base_directory permission to 509
[[34m2025-03-11T12:10:21.344+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:10:21.800+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:10:21.802+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_base_directory, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:10:21.372617+00:00, run_end_date=2025-03-11 10:10:21.458651+00:00, run_duration=0.086034, state=success, executor_state=success, try_number=1, max_tries=1, job_id=17, pool=default_pool, queue=default, priority_weight=10, operator=BashOperator, queued_dttm=2025-03-11 10:10:20.205657+00:00, queued_by_job_id=1, pid=161379[0m
[[34m2025-03-11T12:10:21.948+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:10:21.948+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:10:21.948+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:10:21.949+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task create_data_subdirectories because previous state change time has not been saved[0m
[[34m2025-03-11T12:10:21.949+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_data_subdirectories', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 9 and queue default[0m
[[34m2025-03-11T12:10:21.949+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_data_subdirectories', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:21.950+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_data_subdirectories', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:22.513+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:10:22.593+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:22.629+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:10:22.630+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:22.721+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:10:22.806+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=create_data_subdirectories permission to 509
[[34m2025-03-11T12:10:23.068+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:10:23.533+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_data_subdirectories', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:10:23.536+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_data_subdirectories, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:10:23.099569+00:00, run_end_date=2025-03-11 10:10:23.187824+00:00, run_duration=0.088255, state=success, executor_state=success, try_number=1, max_tries=1, job_id=18, pool=default_pool, queue=default, priority_weight=9, operator=BashOperator, queued_dttm=2025-03-11 10:10:21.948682+00:00, queued_by_job_id=1, pid=161445[0m
[[34m2025-03-11T12:10:23.676+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 3 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:10:23.677+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:10:23.677+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T12:10:23.677+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T12:10:23.677+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:10:23.678+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_beneficiary_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:10:23.678+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_claims_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:10:23.678+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_part_d_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:10:23.678+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_beneficiary_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T12:10:23.678+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_beneficiary_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:23.678+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_claims_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T12:10:23.678+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_claims_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:23.679+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_part_d_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:10:23.679+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_part_d_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:23.680+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_beneficiary_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:24.291+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:10:24.381+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:24.417+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:10:24.418+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:24.502+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:10:24.584+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=download_beneficiary_data permission to 509
[[34m2025-03-11T12:10:24.829+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:10:27.606+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_claims_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:28.244+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:10:28.328+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:28.361+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:10:28.362+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:28.446+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:10:28.538+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=download_claims_data permission to 509
[[34m2025-03-11T12:10:28.840+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:12:05.544+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_part_d_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:12:06.129+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:12:06.212+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:12:06.243+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:12:06.243+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:12:06.322+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:12:06.405+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=download_part_d_data permission to 509
[[34m2025-03-11T12:12:06.695+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:12:33.974+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_beneficiary_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:12:33.975+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_claims_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:12:33.975+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_part_d_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:12:33.978+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_beneficiary_data, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:10:24.857938+00:00, run_end_date=2025-03-11 10:10:27.260744+00:00, run_duration=2.402806, state=success, executor_state=success, try_number=1, max_tries=1, job_id=19, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 10:10:23.677700+00:00, queued_by_job_id=1, pid=161493[0m
[[34m2025-03-11T12:12:33.978+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_claims_data, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:10:28.875744+00:00, run_end_date=2025-03-11 10:12:05.210511+00:00, run_duration=96.334767, state=success, executor_state=success, try_number=1, max_tries=1, job_id=20, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 10:10:23.677700+00:00, queued_by_job_id=1, pid=161611[0m
[[34m2025-03-11T12:12:33.978+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_part_d_data, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:12:06.729695+00:00, run_end_date=2025-03-11 10:12:33.560402+00:00, run_duration=26.830707, state=success, executor_state=success, try_number=1, max_tries=1, job_id=21, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:10:23.677700+00:00, queued_by_job_id=1, pid=164046[0m
[[34m2025-03-11T12:12:33.988+0200[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=160055) last sent a heartbeat 130.33 seconds ago! Restarting it[0m
[[34m2025-03-11T12:12:33.994+0200[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 160055. PIDs of all processes in the group: [160055][0m
[[34m2025-03-11T12:12:33.994+0200[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 160055[0m
[[34m2025-03-11T12:12:34.086+0200[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=160055, status='terminated', exitcode=0, started='12:09:32') (160055) terminated with exit code 0[0m
[[34m2025-03-11T12:12:34.090+0200[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 164727[0m
[[34m2025-03-11T12:12:34.094+0200[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2025-03-11T12:12:34.104+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T12:12:34.106+0200[0m] {[34mscheduler_job_runner.py:[0m1628} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2025-03-11T12:12:34.107+0200] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-03-11T12:12:34.409+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 3 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:12:34.409+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:12:34.409+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T12:12:34.409+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T12:12:34.409+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:12:34.410+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_beneficiary_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:12:34.410+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_claims_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:12:34.410+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task upload_part_d_to_s3 because previous state change time has not been saved[0m
[[34m2025-03-11T12:12:34.410+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_beneficiary_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:12:34.410+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_beneficiary_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:12:34.410+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_claims_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:12:34.410+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_claims_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:12:34.410+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_part_d_to_s3', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-03-11T12:12:34.411+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_part_d_to_s3', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:12:34.412+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_beneficiary_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:12:35.026+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:12:35.107+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:12:35.139+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:12:35.139+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:12:35.217+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:12:35.300+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=extract_beneficiary_data permission to 509
[[34m2025-03-11T12:12:35.544+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:12:36.140+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_claims_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:12:36.751+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:12:36.840+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:12:36.875+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:12:36.875+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:12:36.962+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:12:37.049+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=extract_claims_data permission to 509
[[34m2025-03-11T12:12:37.319+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:12:39.876+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_part_d_to_s3', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:12:40.424+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:12:40.506+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:12:40.543+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:12:40.543+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:12:40.626+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:12:40.711+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=upload_part_d_to_s3 permission to 509
[[34m2025-03-11T12:12:40.979+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:13:08.336+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_beneficiary_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:13:08.336+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_claims_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:13:08.336+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_part_d_to_s3', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:13:08.339+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=extract_beneficiary_data, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:12:35.573017+00:00, run_end_date=2025-03-11 10:12:35.774471+00:00, run_duration=0.201454, state=success, executor_state=success, try_number=1, max_tries=1, job_id=22, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:12:34.409766+00:00, queued_by_job_id=1, pid=164765[0m
[[34m2025-03-11T12:13:08.339+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=extract_claims_data, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:12:37.349934+00:00, run_end_date=2025-03-11 10:12:39.539273+00:00, run_duration=2.189339, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:12:34.409766+00:00, queued_by_job_id=1, pid=164835[0m
[[34m2025-03-11T12:13:08.339+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=upload_part_d_to_s3, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:12:41.012163+00:00, run_end_date=2025-03-11 10:13:08.023744+00:00, run_duration=27.011581, state=success, executor_state=success, try_number=1, max_tries=1, job_id=24, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-03-11 10:12:34.409766+00:00, queued_by_job_id=1, pid=164932[0m
[[34m2025-03-11T12:13:08.509+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 2 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.upload_beneficiary_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_claims_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:13:08.510+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:13:08.510+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T12:13:08.510+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.upload_beneficiary_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_claims_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:13:08.510+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task upload_beneficiary_to_s3 because previous state change time has not been saved[0m
[[34m2025-03-11T12:13:08.511+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task upload_claims_to_s3 because previous state change time has not been saved[0m
[[34m2025-03-11T12:13:08.511+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_beneficiary_to_s3', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-03-11T12:13:08.511+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_beneficiary_to_s3', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:13:08.511+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_claims_to_s3', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-03-11T12:13:08.511+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_claims_to_s3', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:13:08.513+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_beneficiary_to_s3', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:13:09.074+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:13:09.152+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:13:09.184+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:13:09.185+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:13:09.259+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:13:09.347+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=upload_beneficiary_to_s3 permission to 509
[[34m2025-03-11T12:13:09.584+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.upload_beneficiary_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:13:23.931+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_claims_to_s3', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:13:24.551+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:13:24.649+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:13:24.684+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:13:24.685+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:13:24.762+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:13:24.846+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=upload_claims_to_s3 permission to 509
[[34m2025-03-11T12:13:25.107+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.upload_claims_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
