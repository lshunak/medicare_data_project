[[34m2025-03-11T11:55:30.555+0200[0m] {[34mscheduler_job_runner.py:[0m797} INFO[0m - Starting the scheduler[0m
[[34m2025-03-11T11:55:30.555+0200[0m] {[34mscheduler_job_runner.py:[0m804} INFO[0m - Processing each file at most -1 times[0m
[[34m2025-03-11T11:55:30.559+0200[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 138902[0m
[[34m2025-03-11T11:55:30.560+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T11:55:30.562+0200[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2025-03-11T11:55:30.573+0200] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-03-11T12:00:06.140+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:00:06.140+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:00:06.140+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:00:06.141+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task create_base_directory because previous state change time has not been saved[0m
[[34m2025-03-11T12:00:06.142+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2025-03-11T12:00:06.142+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:00:06.144+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:00:06.679+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:00:06.767+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:00:06.796+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:00:06.796+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:00:06.877+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:00:06.954+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:00:04.657498+00:00/task_id=create_base_directory permission to 509
[[34m2025-03-11T12:00:07.193+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:00:07.604+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:00:07.608+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_base_directory, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:00:07.219429+00:00, run_end_date=2025-03-11 10:00:07.289845+00:00, run_duration=0.070416, state=up_for_retry, executor_state=success, try_number=1, max_tries=1, job_id=2, pool=default_pool, queue=default, priority_weight=10, operator=BashOperator, queued_dttm=2025-03-11 10:00:06.141152+00:00, queued_by_job_id=1, pid=145609[0m
[[34m2025-03-11T12:00:30.744+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T12:01:35.925+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:01:35.926+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:01:35.926+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:01:35.927+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=2, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2025-03-11T12:01:35.927+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:01:35.929+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:01:36.558+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:01:36.654+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:01:36.691+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:01:36.691+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:01:36.794+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:01:36.908+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-03-11T12:01:37.214+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:01:37.675+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=2, map_index=-1)[0m
[[34m2025-03-11T12:01:37.678+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_base_directory, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:01:37.244845+00:00, run_end_date=2025-03-11 10:01:37.327051+00:00, run_duration=0.082206, state=up_for_retry, executor_state=success, try_number=2, max_tries=2, job_id=3, pool=default_pool, queue=default, priority_weight=10, operator=BashOperator, queued_dttm=2025-03-11 10:01:35.926781+00:00, queued_by_job_id=1, pid=147921[0m
[[34m2025-03-11T12:01:57.975+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:01:57.977+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:01:57.977+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:01:57.978+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.upload_beneficiary_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:01:57.978+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:01:57.979+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:01:57.979+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.upload_claims_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:01:57.979+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:01:57.980+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:04:25.703+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.703+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.703+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.703+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.upload_beneficiary_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.703+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.704+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.704+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.upload_claims_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.704+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.704+0200[0m] {[34mdagrun.py:[0m1028} INFO[0m - Restoring task '<TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [removed]>' which was previously removed from DAG '<DAG: medicare_data_extract_s3>'[0m
[[34m2025-03-11T12:04:25.707+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.download_data manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:04:25.709+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.process_data manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:04:25.709+0200[0m] {[34mdagrun.py:[0m743} ERROR[0m - Failed to get task for ti <TaskInstance: medicare_data_extract_s3.upload_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [None]>. Marking it as removed.[0m
[[34m2025-03-11T12:05:30.818+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2025-03-11T12:06:34.249+0200] {manager.py:543} INFO - DAG medicare_data_extract_s3 is missing and will be deactivated.
[2025-03-11T12:06:34.253+0200] {manager.py:553} INFO - Deactivated 1 DAGs which are no longer present in file.
[2025-03-11T12:06:34.262+0200] {manager.py:557} INFO - Deleted DAG medicare_data_extract_s3 in serialized_dag table
[[34m2025-03-11T12:07:06.905+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:07:06.905+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:07:06.906+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:07:06.907+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=3, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2025-03-11T12:07:06.907+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:06.908+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:07.627+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:07:07.747+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:07:07.802+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:07:07.802+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:07:07.897+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:07:08.025+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-03-11T12:07:08.338+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:07:08.820+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=3, map_index=-1)[0m
[[34m2025-03-11T12:07:08.824+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_base_directory, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:07:08.369923+00:00, run_end_date=2025-03-11 10:07:08.455853+00:00, run_duration=0.08593, state=success, executor_state=success, try_number=3, max_tries=2, job_id=4, pool=default_pool, queue=default, priority_weight=10, operator=BashOperator, queued_dttm=2025-03-11 10:07:06.906415+00:00, queued_by_job_id=1, pid=156437[0m
[[34m2025-03-11T12:07:08.985+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:07:08.985+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:07:08.985+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:07:08.986+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task create_data_subdirectories because previous state change time has not been saved[0m
[[34m2025-03-11T12:07:08.986+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_data_subdirectories', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1) to executor with priority 9 and queue default[0m
[[34m2025-03-11T12:07:08.987+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_data_subdirectories', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:08.988+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_data_subdirectories', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:09.679+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:07:09.780+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:07:09.830+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:07:09.830+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:07:09.926+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:07:10.020+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:00:04.657498+00:00/task_id=create_data_subdirectories permission to 509
[[34m2025-03-11T12:07:10.301+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:07:10.950+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_data_subdirectories', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:07:10.955+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_data_subdirectories, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:07:10.334539+00:00, run_end_date=2025-03-11 10:07:10.422996+00:00, run_duration=0.088457, state=success, executor_state=success, try_number=1, max_tries=1, job_id=5, pool=default_pool, queue=default, priority_weight=9, operator=BashOperator, queued_dttm=2025-03-11 10:07:08.985947+00:00, queued_by_job_id=1, pid=156491[0m
[[34m2025-03-11T12:07:11.400+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 3 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:07:11.400+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:07:11.400+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T12:07:11.400+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T12:07:11.400+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:07:11.401+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_beneficiary_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:07:11.401+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_claims_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:07:11.402+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_part_d_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:07:11.402+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_beneficiary_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T12:07:11.402+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_beneficiary_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:11.402+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_claims_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T12:07:11.402+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_claims_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:11.402+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_part_d_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:07:11.402+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_part_d_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:11.404+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_beneficiary_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:12.115+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:07:12.226+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:07:12.272+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:07:12.273+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:07:12.381+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:07:12.496+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:00:04.657498+00:00/task_id=download_beneficiary_data permission to 509
[[34m2025-03-11T12:07:12.846+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:07:17.613+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_claims_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:07:18.311+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:07:18.398+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:07:18.432+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:07:18.432+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:07:18.512+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:07:18.607+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:00:04.657498+00:00/task_id=download_claims_data permission to 509
[[34m2025-03-11T12:07:18.870+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:09:16.674+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_part_d_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:17.256+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:09:17.331+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:17.362+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:09:17.362+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:17.438+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:09:17.522+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:00:04.657498+00:00/task_id=download_part_d_data permission to 509
[[34m2025-03-11T12:09:17.763+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:09:32.649+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_beneficiary_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:09:32.650+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_claims_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:09:32.650+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_part_d_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:09:32.652+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_beneficiary_data, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:07:12.887957+00:00, run_end_date=2025-03-11 10:07:17.156749+00:00, run_duration=4.268792, state=success, executor_state=success, try_number=1, max_tries=1, job_id=6, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 10:07:11.401165+00:00, queued_by_job_id=1, pid=156567[0m
[[34m2025-03-11T12:09:32.652+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_claims_data, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:07:18.900911+00:00, run_end_date=2025-03-11 10:09:16.325259+00:00, run_duration=117.424348, state=success, executor_state=success, try_number=1, max_tries=1, job_id=7, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 10:07:11.401165+00:00, queued_by_job_id=1, pid=156703[0m
[[34m2025-03-11T12:09:32.652+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_part_d_data, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:09:17.790406+00:00, run_end_date=2025-03-11 10:09:32.301508+00:00, run_duration=14.511102, state=success, executor_state=success, try_number=1, max_tries=1, job_id=8, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:07:11.401165+00:00, queued_by_job_id=1, pid=159693[0m
[[34m2025-03-11T12:09:32.663+0200[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=138902) last sent a heartbeat 141.29 seconds ago! Restarting it[0m
[[34m2025-03-11T12:09:32.667+0200[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 138902. PIDs of all processes in the group: [138902][0m
[[34m2025-03-11T12:09:32.668+0200[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 138902[0m
[[34m2025-03-11T12:09:32.760+0200[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=138902, status='terminated', exitcode=0, started='11:55:30') (138902) terminated with exit code 0[0m
[[34m2025-03-11T12:09:32.764+0200[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 160055[0m
[[34m2025-03-11T12:09:32.770+0200[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2025-03-11T12:09:32.786+0200] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-03-11T12:09:33.088+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 4 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:09:33.088+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:09:33.089+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T12:09:33.089+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T12:09:33.089+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 3/16 running and queued tasks[0m
[[34m2025-03-11T12:09:33.089+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [scheduled]>[0m
[[34m2025-03-11T12:09:33.090+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task create_base_directory because previous state change time has not been saved[0m
[[34m2025-03-11T12:09:33.090+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_beneficiary_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:09:33.090+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_claims_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task upload_part_d_to_s3 because previous state change time has not been saved[0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_beneficiary_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_beneficiary_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_claims_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_claims_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_part_d_to_s3', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-03-11T12:09:33.091+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_part_d_to_s3', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:33.093+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:33.599+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:09:33.686+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:33.720+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:09:33.720+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:33.800+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:09:33.882+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:07:28.075748+00:00/task_id=create_base_directory permission to 509
[[34m2025-03-11T12:09:34.124+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:07:28.075748+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:09:34.538+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_beneficiary_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:35.069+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:09:35.151+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:35.181+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:09:35.182+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:35.255+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:09:35.336+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:00:04.657498+00:00/task_id=extract_beneficiary_data permission to 509
[[34m2025-03-11T12:09:35.578+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:09:36.078+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_claims_data', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:36.628+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:09:36.718+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:36.768+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:09:36.769+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:36.867+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:09:36.948+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:00:04.657498+00:00/task_id=extract_claims_data permission to 509
[[34m2025-03-11T12:09:37.203+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:09:39.415+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_part_d_to_s3', 'manual__2025-03-11T10:00:04.657498+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:39.948+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:09:40.032+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:40.066+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:09:40.066+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:40.147+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:09:40.229+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:00:04.657498+00:00/task_id=upload_part_d_to_s3 permission to 509
[[34m2025-03-11T12:09:40.469+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:00:04.657498+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:09:54.409+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:09:54.409+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_beneficiary_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:09:54.409+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_claims_data', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:09:54.409+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_part_d_to_s3', run_id='manual__2025-03-11T10:00:04.657498+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:09:54.415+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_base_directory, run_id=manual__2025-03-11T10:07:28.075748+00:00, map_index=-1, run_start_date=2025-03-11 10:09:34.152882+00:00, run_end_date=2025-03-11 10:09:34.229432+00:00, run_duration=0.07655, state=success, executor_state=success, try_number=1, max_tries=1, job_id=9, pool=default_pool, queue=default, priority_weight=10, operator=BashOperator, queued_dttm=2025-03-11 10:09:33.089833+00:00, queued_by_job_id=1, pid=160100[0m
[[34m2025-03-11T12:09:54.415+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=extract_beneficiary_data, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:09:35.606107+00:00, run_end_date=2025-03-11 10:09:35.774450+00:00, run_duration=0.168343, state=success, executor_state=success, try_number=1, max_tries=1, job_id=10, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:09:33.089833+00:00, queued_by_job_id=1, pid=160137[0m
[[34m2025-03-11T12:09:54.415+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=extract_claims_data, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:09:37.230442+00:00, run_end_date=2025-03-11 10:09:39.105540+00:00, run_duration=1.875098, state=success, executor_state=success, try_number=1, max_tries=1, job_id=11, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:09:33.089833+00:00, queued_by_job_id=1, pid=160178[0m
[[34m2025-03-11T12:09:54.415+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=upload_part_d_to_s3, run_id=manual__2025-03-11T10:00:04.657498+00:00, map_index=-1, run_start_date=2025-03-11 10:09:40.491658+00:00, run_end_date=2025-03-11 10:09:44.816970+00:00, run_duration=4.325312, state=failed, executor_state=success, try_number=1, max_tries=1, job_id=12, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-03-11 10:09:33.089833+00:00, queued_by_job_id=1, pid=160281[0m
[[34m2025-03-11T12:09:54.570+0200[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun medicare_data_extract_s3 @ 2025-03-11 10:00:04.657498+00:00: manual__2025-03-11T10:00:04.657498+00:00, state:running, queued_at: 2025-03-11 10:00:04.676355+00:00. externally triggered: True> failed[0m
[[34m2025-03-11T12:09:54.571+0200[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=medicare_data_extract_s3, execution_date=2025-03-11 10:00:04.657498+00:00, run_id=manual__2025-03-11T10:00:04.657498+00:00, run_start_date=2025-03-11 10:00:06.107714+00:00, run_end_date=2025-03-11 10:09:54.571456+00:00, run_duration=588.463742, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-03-11 10:00:04.657498+00:00, data_interval_end=2025-03-11 10:00:04.657498+00:00, dag_hash=6476a35378d0bd98617ac0ab63c3bf83[0m
[[34m2025-03-11T12:09:54.578+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>[0m
[[34m2025-03-11T12:09:54.579+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:09:54.579+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>[0m
[[34m2025-03-11T12:09:54.580+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task create_data_subdirectories because previous state change time has not been saved[0m
[[34m2025-03-11T12:09:54.580+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_data_subdirectories', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1) to executor with priority 9 and queue default[0m
[[34m2025-03-11T12:09:54.580+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_data_subdirectories', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:54.582+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_data_subdirectories', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:55.245+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:09:55.335+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:55.371+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:09:55.371+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:55.453+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:09:55.543+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:07:28.075748+00:00/task_id=create_data_subdirectories permission to 509
[[34m2025-03-11T12:09:55.805+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:07:28.075748+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:09:56.253+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_data_subdirectories', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:09:56.256+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_data_subdirectories, run_id=manual__2025-03-11T10:07:28.075748+00:00, map_index=-1, run_start_date=2025-03-11 10:09:55.835814+00:00, run_end_date=2025-03-11 10:09:55.923177+00:00, run_duration=0.087363, state=success, executor_state=success, try_number=1, max_tries=1, job_id=13, pool=default_pool, queue=default, priority_weight=9, operator=BashOperator, queued_dttm=2025-03-11 10:09:54.579527+00:00, queued_by_job_id=1, pid=160702[0m
[[34m2025-03-11T12:09:56.397+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 3 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>[0m
[[34m2025-03-11T12:09:56.397+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:09:56.397+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T12:09:56.397+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T12:09:56.397+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:07:28.075748+00:00 [scheduled]>[0m
[[34m2025-03-11T12:09:56.398+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_beneficiary_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:09:56.398+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_claims_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:09:56.398+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_part_d_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:09:56.398+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_beneficiary_data', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T12:09:56.398+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_beneficiary_data', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:56.398+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_claims_data', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T12:09:56.398+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_claims_data', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:56.398+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_part_d_data', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:09:56.399+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_part_d_data', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:56.400+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_beneficiary_data', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:09:56.961+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:09:57.041+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:57.073+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:09:57.073+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:09:57.155+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:09:57.241+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:07:28.075748+00:00/task_id=download_beneficiary_data permission to 509
[[34m2025-03-11T12:09:57.478+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:07:28.075748+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:10:03.206+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_claims_data', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:03.781+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:10:03.871+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:03.905+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:10:03.906+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:03.983+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:10:04.062+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:07:28.075748+00:00/task_id=download_claims_data permission to 509
[[34m2025-03-11T12:10:04.315+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:07:28.075748+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:10:14.807+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_part_d_data', 'manual__2025-03-11T10:07:28.075748+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:15.564+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:10:15.659+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:15.694+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:10:15.694+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:15.771+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:10:15.850+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:07:28.075748+00:00/task_id=download_part_d_data permission to 509
[[34m2025-03-11T12:10:16.087+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:07:28.075748+00:00 [skipped]> on host liran-Zenbook[0m
[[34m2025-03-11T12:10:16.389+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_beneficiary_data', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:10:16.389+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_claims_data', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:10:16.389+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_part_d_data', run_id='manual__2025-03-11T10:07:28.075748+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:10:16.392+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_beneficiary_data, run_id=manual__2025-03-11T10:07:28.075748+00:00, map_index=-1, run_start_date=2025-03-11 10:09:57.505071+00:00, run_end_date=2025-03-11 10:10:02.893886+00:00, run_duration=5.388815, state=success, executor_state=success, try_number=1, max_tries=1, job_id=14, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 10:09:56.397906+00:00, queued_by_job_id=1, pid=160744[0m
[[34m2025-03-11T12:10:16.392+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_claims_data, run_id=manual__2025-03-11T10:07:28.075748+00:00, map_index=-1, run_start_date=2025-03-11 10:10:04.341711+00:00, run_end_date=2025-03-11 10:10:04.720426+00:00, run_duration=0.378715, state=failed, executor_state=success, try_number=1, max_tries=1, job_id=15, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 10:09:56.397906+00:00, queued_by_job_id=1, pid=160913[0m
[[34m2025-03-11T12:10:16.392+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_part_d_data, run_id=manual__2025-03-11T10:07:28.075748+00:00, map_index=-1, run_start_date=2025-03-11 10:10:04.709523+00:00, run_end_date=2025-03-11 10:10:04.709523+00:00, run_duration=0.0, state=skipped, executor_state=success, try_number=1, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:09:56.397906+00:00, queued_by_job_id=1, pid=None[0m
[[34m2025-03-11T12:10:19.035+0200[0m] {[34mdagrun.py:[0m632} ERROR[0m - Marking run <DagRun medicare_data_extract_s3 @ 2025-03-11 10:07:28.075748+00:00: manual__2025-03-11T10:07:28.075748+00:00, state:running, queued_at: 2025-03-11 10:07:28.085286+00:00. externally triggered: True> failed[0m
[[34m2025-03-11T12:10:19.035+0200[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=medicare_data_extract_s3, execution_date=2025-03-11 10:07:28.075748+00:00, run_id=manual__2025-03-11T10:07:28.075748+00:00, run_start_date=2025-03-11 10:09:33.068474+00:00, run_end_date=2025-03-11 10:10:19.035724+00:00, run_duration=45.96725, state=failed, external_trigger=True, run_type=manual, data_interval_start=2025-03-11 10:07:28.075748+00:00, data_interval_end=2025-03-11 10:07:28.075748+00:00, dag_hash=6476a35378d0bd98617ac0ab63c3bf83[0m
[[34m2025-03-11T12:10:20.205+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:10:20.205+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:10:20.205+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:10:20.206+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task create_base_directory because previous state change time has not been saved[0m
[[34m2025-03-11T12:10:20.206+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 10 and queue default[0m
[[34m2025-03-11T12:10:20.206+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:20.207+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:20.780+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:10:20.870+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:20.903+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:10:20.903+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:20.984+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:10:21.073+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=create_base_directory permission to 509
[[34m2025-03-11T12:10:21.344+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:10:21.800+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:10:21.802+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_base_directory, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:10:21.372617+00:00, run_end_date=2025-03-11 10:10:21.458651+00:00, run_duration=0.086034, state=success, executor_state=success, try_number=1, max_tries=1, job_id=17, pool=default_pool, queue=default, priority_weight=10, operator=BashOperator, queued_dttm=2025-03-11 10:10:20.205657+00:00, queued_by_job_id=1, pid=161379[0m
[[34m2025-03-11T12:10:21.948+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:10:21.948+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:10:21.948+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:10:21.949+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task create_data_subdirectories because previous state change time has not been saved[0m
[[34m2025-03-11T12:10:21.949+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_data_subdirectories', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 9 and queue default[0m
[[34m2025-03-11T12:10:21.949+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_data_subdirectories', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:21.950+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_data_subdirectories', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:22.513+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:10:22.593+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:22.629+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:10:22.630+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:22.721+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:10:22.806+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=create_data_subdirectories permission to 509
[[34m2025-03-11T12:10:23.068+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:10:23.533+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_data_subdirectories', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:10:23.536+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_data_subdirectories, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:10:23.099569+00:00, run_end_date=2025-03-11 10:10:23.187824+00:00, run_duration=0.088255, state=success, executor_state=success, try_number=1, max_tries=1, job_id=18, pool=default_pool, queue=default, priority_weight=9, operator=BashOperator, queued_dttm=2025-03-11 10:10:21.948682+00:00, queued_by_job_id=1, pid=161445[0m
[[34m2025-03-11T12:10:23.676+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 3 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:10:23.677+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:10:23.677+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T12:10:23.677+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T12:10:23.677+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:10:23.678+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_beneficiary_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:10:23.678+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_claims_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:10:23.678+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_part_d_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:10:23.678+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_beneficiary_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T12:10:23.678+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_beneficiary_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:23.678+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_claims_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T12:10:23.678+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_claims_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:23.679+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_part_d_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:10:23.679+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_part_d_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:23.680+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_beneficiary_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:24.291+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:10:24.381+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:24.417+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:10:24.418+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:24.502+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:10:24.584+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=download_beneficiary_data permission to 509
[[34m2025-03-11T12:10:24.829+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:10:27.606+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_claims_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:10:28.244+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:10:28.328+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:28.361+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:10:28.362+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:10:28.446+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:10:28.538+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=download_claims_data permission to 509
[[34m2025-03-11T12:10:28.840+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:12:05.544+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_part_d_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:12:06.129+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:12:06.212+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:12:06.243+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:12:06.243+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:12:06.322+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:12:06.405+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=download_part_d_data permission to 509
[[34m2025-03-11T12:12:06.695+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:12:33.974+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_beneficiary_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:12:33.975+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_claims_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:12:33.975+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_part_d_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:12:33.978+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_beneficiary_data, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:10:24.857938+00:00, run_end_date=2025-03-11 10:10:27.260744+00:00, run_duration=2.402806, state=success, executor_state=success, try_number=1, max_tries=1, job_id=19, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 10:10:23.677700+00:00, queued_by_job_id=1, pid=161493[0m
[[34m2025-03-11T12:12:33.978+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_claims_data, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:10:28.875744+00:00, run_end_date=2025-03-11 10:12:05.210511+00:00, run_duration=96.334767, state=success, executor_state=success, try_number=1, max_tries=1, job_id=20, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 10:10:23.677700+00:00, queued_by_job_id=1, pid=161611[0m
[[34m2025-03-11T12:12:33.978+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_part_d_data, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:12:06.729695+00:00, run_end_date=2025-03-11 10:12:33.560402+00:00, run_duration=26.830707, state=success, executor_state=success, try_number=1, max_tries=1, job_id=21, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:10:23.677700+00:00, queued_by_job_id=1, pid=164046[0m
[[34m2025-03-11T12:12:33.988+0200[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=160055) last sent a heartbeat 130.33 seconds ago! Restarting it[0m
[[34m2025-03-11T12:12:33.994+0200[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 160055. PIDs of all processes in the group: [160055][0m
[[34m2025-03-11T12:12:33.994+0200[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 160055[0m
[[34m2025-03-11T12:12:34.086+0200[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=160055, status='terminated', exitcode=0, started='12:09:32') (160055) terminated with exit code 0[0m
[[34m2025-03-11T12:12:34.090+0200[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 164727[0m
[[34m2025-03-11T12:12:34.094+0200[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2025-03-11T12:12:34.104+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T12:12:34.106+0200[0m] {[34mscheduler_job_runner.py:[0m1628} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2025-03-11T12:12:34.107+0200] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-03-11T12:12:34.409+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 3 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:12:34.409+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:12:34.409+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T12:12:34.409+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T12:12:34.409+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:12:34.410+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_beneficiary_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:12:34.410+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_claims_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:12:34.410+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task upload_part_d_to_s3 because previous state change time has not been saved[0m
[[34m2025-03-11T12:12:34.410+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_beneficiary_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:12:34.410+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_beneficiary_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:12:34.410+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_claims_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:12:34.410+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_claims_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:12:34.410+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_part_d_to_s3', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-03-11T12:12:34.411+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_part_d_to_s3', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:12:34.412+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_beneficiary_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:12:35.026+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:12:35.107+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:12:35.139+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:12:35.139+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:12:35.217+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:12:35.300+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=extract_beneficiary_data permission to 509
[[34m2025-03-11T12:12:35.544+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:12:36.140+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_claims_data', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:12:36.751+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:12:36.840+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:12:36.875+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:12:36.875+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:12:36.962+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:12:37.049+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=extract_claims_data permission to 509
[[34m2025-03-11T12:12:37.319+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:12:39.876+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_part_d_to_s3', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:12:40.424+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:12:40.506+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:12:40.543+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:12:40.543+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:12:40.626+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:12:40.711+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=upload_part_d_to_s3 permission to 509
[[34m2025-03-11T12:12:40.979+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:13:08.336+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_beneficiary_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:13:08.336+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_claims_data', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:13:08.336+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_part_d_to_s3', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:13:08.339+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=extract_beneficiary_data, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:12:35.573017+00:00, run_end_date=2025-03-11 10:12:35.774471+00:00, run_duration=0.201454, state=success, executor_state=success, try_number=1, max_tries=1, job_id=22, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:12:34.409766+00:00, queued_by_job_id=1, pid=164765[0m
[[34m2025-03-11T12:13:08.339+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=extract_claims_data, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:12:37.349934+00:00, run_end_date=2025-03-11 10:12:39.539273+00:00, run_duration=2.189339, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:12:34.409766+00:00, queued_by_job_id=1, pid=164835[0m
[[34m2025-03-11T12:13:08.339+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=upload_part_d_to_s3, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:12:41.012163+00:00, run_end_date=2025-03-11 10:13:08.023744+00:00, run_duration=27.011581, state=success, executor_state=success, try_number=1, max_tries=1, job_id=24, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-03-11 10:12:34.409766+00:00, queued_by_job_id=1, pid=164932[0m
[[34m2025-03-11T12:13:08.509+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 2 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.upload_beneficiary_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_claims_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:13:08.510+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:13:08.510+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T12:13:08.510+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.upload_beneficiary_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_claims_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [scheduled]>[0m
[[34m2025-03-11T12:13:08.510+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task upload_beneficiary_to_s3 because previous state change time has not been saved[0m
[[34m2025-03-11T12:13:08.511+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task upload_claims_to_s3 because previous state change time has not been saved[0m
[[34m2025-03-11T12:13:08.511+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_beneficiary_to_s3', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-03-11T12:13:08.511+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_beneficiary_to_s3', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:13:08.511+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_claims_to_s3', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-03-11T12:13:08.511+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_claims_to_s3', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:13:08.513+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_beneficiary_to_s3', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:13:09.074+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:13:09.152+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:13:09.184+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:13:09.185+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:13:09.259+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:13:09.347+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=upload_beneficiary_to_s3 permission to 509
[[34m2025-03-11T12:13:09.584+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.upload_beneficiary_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:13:23.931+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_claims_to_s3', 'manual__2025-03-11T10:10:19.387762+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:13:24.551+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:13:24.649+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:13:24.684+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:13:24.685+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:13:24.762+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:13:24.846+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:10:19.387762+00:00/task_id=upload_claims_to_s3 permission to 509
[[34m2025-03-11T12:13:25.107+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.upload_claims_to_s3 manual__2025-03-11T10:10:19.387762+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:17:58.083+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_beneficiary_to_s3', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:17:58.084+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_claims_to_s3', run_id='manual__2025-03-11T10:10:19.387762+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:17:58.086+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=upload_beneficiary_to_s3, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:13:09.609672+00:00, run_end_date=2025-03-11 10:13:23.556800+00:00, run_duration=13.947128, state=success, executor_state=success, try_number=1, max_tries=1, job_id=25, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-03-11 10:13:08.510437+00:00, queued_by_job_id=1, pid=165569[0m
[[34m2025-03-11T12:17:58.086+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=upload_claims_to_s3, run_id=manual__2025-03-11T10:10:19.387762+00:00, map_index=-1, run_start_date=2025-03-11 10:13:25.136018+00:00, run_end_date=2025-03-11 10:17:57.787520+00:00, run_duration=272.651502, state=success, executor_state=success, try_number=1, max_tries=1, job_id=26, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-03-11 10:13:08.510437+00:00, queued_by_job_id=1, pid=165916[0m
[[34m2025-03-11T12:17:58.097+0200[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=164727) last sent a heartbeat 289.61 seconds ago! Restarting it[0m
[[34m2025-03-11T12:17:58.102+0200[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 164727. PIDs of all processes in the group: [164727][0m
[[34m2025-03-11T12:17:58.102+0200[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 164727[0m
[[34m2025-03-11T12:17:58.194+0200[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=164727, status='terminated', exitcode=0, started='12:12:33') (164727) terminated with exit code 0[0m
[[34m2025-03-11T12:17:58.198+0200[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 172849[0m
[[34m2025-03-11T12:17:58.201+0200[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2025-03-11T12:17:58.214+0200] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-03-11T12:17:58.222+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T12:17:58.224+0200[0m] {[34mscheduler_job_runner.py:[0m1628} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2025-03-11T12:17:58.487+0200[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun medicare_data_extract_s3 @ 2025-03-11 10:10:19.387762+00:00: manual__2025-03-11T10:10:19.387762+00:00, state:running, queued_at: 2025-03-11 10:10:19.392346+00:00. externally triggered: True> successful[0m
[[34m2025-03-11T12:17:58.487+0200[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=medicare_data_extract_s3, execution_date=2025-03-11 10:10:19.387762+00:00, run_id=manual__2025-03-11T10:10:19.387762+00:00, run_start_date=2025-03-11 10:10:20.178699+00:00, run_end_date=2025-03-11 10:17:58.487340+00:00, run_duration=458.308641, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-03-11 10:10:19.387762+00:00, data_interval_end=2025-03-11 10:10:19.387762+00:00, dag_hash=6476a35378d0bd98617ac0ab63c3bf83[0m
[[34m2025-03-11T12:22:58.395+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T12:27:58.554+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T12:29:09.351+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>[0m
[[34m2025-03-11T12:29:09.351+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:29:09.351+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>[0m
[[34m2025-03-11T12:29:09.352+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task create_base_directory because previous state change time has not been saved[0m
[[34m2025-03-11T12:29:09.352+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1) to executor with priority 13 and queue default[0m
[[34m2025-03-11T12:29:09.352+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:29:09.354+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:29:09.907+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:29:09.997+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:29:10.033+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:29:10.033+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:29:10.115+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:29:10.208+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:29:08.321231+00:00/task_id=create_base_directory permission to 509
[[34m2025-03-11T12:29:10.457+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T10:29:08.321231+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:29:10.886+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:29:10.888+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_base_directory, run_id=manual__2025-03-11T10:29:08.321231+00:00, map_index=-1, run_start_date=2025-03-11 10:29:10.488011+00:00, run_end_date=2025-03-11 10:29:10.567350+00:00, run_duration=0.079339, state=success, executor_state=success, try_number=1, max_tries=1, job_id=27, pool=default_pool, queue=default, priority_weight=13, operator=BashOperator, queued_dttm=2025-03-11 10:29:09.351755+00:00, queued_by_job_id=1, pid=186231[0m
[[34m2025-03-11T12:29:11.024+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>[0m
[[34m2025-03-11T12:29:11.024+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:29:11.024+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>[0m
[[34m2025-03-11T12:29:11.025+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task create_data_subdirectories because previous state change time has not been saved[0m
[[34m2025-03-11T12:29:11.025+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_data_subdirectories', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1) to executor with priority 12 and queue default[0m
[[34m2025-03-11T12:29:11.026+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_data_subdirectories', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:29:11.028+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_data_subdirectories', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:29:11.580+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:29:11.668+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:29:11.699+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:29:11.699+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:29:11.778+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:29:11.859+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:29:08.321231+00:00/task_id=create_data_subdirectories permission to 509
[[34m2025-03-11T12:29:12.100+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T10:29:08.321231+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:29:12.577+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_data_subdirectories', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:29:12.580+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_data_subdirectories, run_id=manual__2025-03-11T10:29:08.321231+00:00, map_index=-1, run_start_date=2025-03-11 10:29:12.130847+00:00, run_end_date=2025-03-11 10:29:12.207795+00:00, run_duration=0.076948, state=success, executor_state=success, try_number=1, max_tries=1, job_id=28, pool=default_pool, queue=default, priority_weight=12, operator=BashOperator, queued_dttm=2025-03-11 10:29:11.025205+00:00, queued_by_job_id=1, pid=186284[0m
[[34m2025-03-11T12:29:12.737+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 3 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>[0m
[[34m2025-03-11T12:29:12.737+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:29:12.737+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T12:29:12.737+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T12:29:12.737+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>[0m
[[34m2025-03-11T12:29:12.738+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_beneficiary_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:29:12.738+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_claims_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:29:12.738+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_part_d_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:29:12.738+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_beneficiary_data', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-03-11T12:29:12.738+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_beneficiary_data', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:29:12.739+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_claims_data', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-03-11T12:29:12.739+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_claims_data', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:29:12.739+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_part_d_data', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T12:29:12.739+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_part_d_data', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:29:12.740+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_beneficiary_data', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:29:13.282+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:29:13.362+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:29:13.395+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:29:13.395+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:29:13.472+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:29:13.556+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:29:08.321231+00:00/task_id=download_beneficiary_data permission to 509
[[34m2025-03-11T12:29:13.799+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T10:29:08.321231+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:29:16.422+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_claims_data', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:29:17.024+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:29:17.116+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:29:17.150+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:29:17.150+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:29:17.230+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:29:17.315+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:29:08.321231+00:00/task_id=download_claims_data permission to 509
[[34m2025-03-11T12:29:17.583+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T10:29:08.321231+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:29:24.756+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_part_d_data', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:29:25.436+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:29:25.535+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:29:25.576+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:29:25.576+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:29:25.671+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:29:25.764+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:29:08.321231+00:00/task_id=download_part_d_data permission to 509
[[34m2025-03-11T12:29:26.030+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T10:29:08.321231+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:29:30.404+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_beneficiary_data', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:29:30.404+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_claims_data', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:29:30.404+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_part_d_data', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:29:30.407+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_beneficiary_data, run_id=manual__2025-03-11T10:29:08.321231+00:00, map_index=-1, run_start_date=2025-03-11 10:29:13.829947+00:00, run_end_date=2025-03-11 10:29:16.055854+00:00, run_duration=2.225907, state=success, executor_state=success, try_number=1, max_tries=1, job_id=29, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2025-03-11 10:29:12.738092+00:00, queued_by_job_id=1, pid=186331[0m
[[34m2025-03-11T12:29:30.407+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_claims_data, run_id=manual__2025-03-11T10:29:08.321231+00:00, map_index=-1, run_start_date=2025-03-11 10:29:17.617766+00:00, run_end_date=2025-03-11 10:29:24.405402+00:00, run_duration=6.787636, state=success, executor_state=success, try_number=1, max_tries=1, job_id=30, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2025-03-11 10:29:12.738092+00:00, queued_by_job_id=1, pid=186448[0m
[[34m2025-03-11T12:29:30.407+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_part_d_data, run_id=manual__2025-03-11T10:29:08.321231+00:00, map_index=-1, run_start_date=2025-03-11 10:29:26.064340+00:00, run_end_date=2025-03-11 10:29:30.090780+00:00, run_duration=4.02644, state=success, executor_state=success, try_number=1, max_tries=1, job_id=31, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 10:29:12.738092+00:00, queued_by_job_id=1, pid=186672[0m
[[34m2025-03-11T12:29:30.574+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 3 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>[0m
[[34m2025-03-11T12:29:30.574+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:29:30.574+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T12:29:30.574+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T12:29:30.575+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>[0m
[[34m2025-03-11T12:29:30.575+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_beneficiary_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:29:30.576+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_claims_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:29:30.576+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task upload_part_d_to_s3 because previous state change time has not been saved[0m
[[34m2025-03-11T12:29:30.576+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_beneficiary_data', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T12:29:30.576+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_beneficiary_data', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:29:30.576+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_claims_data', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T12:29:30.576+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_claims_data', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:29:30.576+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_part_d_to_s3', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:29:30.576+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_part_d_to_s3', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:29:30.578+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_beneficiary_data', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:29:31.183+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:29:31.286+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:29:31.322+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:29:31.322+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:29:31.413+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:29:31.505+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:29:08.321231+00:00/task_id=extract_beneficiary_data permission to 509
[[34m2025-03-11T12:29:31.771+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T10:29:08.321231+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:29:32.369+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_claims_data', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:29:32.963+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:29:33.047+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:29:33.080+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:29:33.080+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:29:33.160+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:29:33.253+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:29:08.321231+00:00/task_id=extract_claims_data permission to 509
[[34m2025-03-11T12:29:33.501+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T10:29:08.321231+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:29:36.042+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_part_d_to_s3', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:29:36.644+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:29:36.753+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:29:36.797+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:29:36.798+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:29:36.889+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:29:36.975+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:29:08.321231+00:00/task_id=upload_part_d_to_s3 permission to 509
[[34m2025-03-11T12:29:37.218+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T10:29:08.321231+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:30:04.102+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_beneficiary_data', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:30:04.103+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_claims_data', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:30:04.103+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_part_d_to_s3', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:30:04.106+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=extract_beneficiary_data, run_id=manual__2025-03-11T10:29:08.321231+00:00, map_index=-1, run_start_date=2025-03-11 10:29:31.806052+00:00, run_end_date=2025-03-11 10:29:31.996746+00:00, run_duration=0.190694, state=success, executor_state=success, try_number=1, max_tries=1, job_id=32, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 10:29:30.575382+00:00, queued_by_job_id=1, pid=186825[0m
[[34m2025-03-11T12:30:04.106+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=extract_claims_data, run_id=manual__2025-03-11T10:29:08.321231+00:00, map_index=-1, run_start_date=2025-03-11 10:29:33.533037+00:00, run_end_date=2025-03-11 10:29:35.708584+00:00, run_duration=2.175547, state=success, executor_state=success, try_number=1, max_tries=1, job_id=33, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 10:29:30.575382+00:00, queued_by_job_id=1, pid=186868[0m
[[34m2025-03-11T12:30:04.106+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=upload_part_d_to_s3, run_id=manual__2025-03-11T10:29:08.321231+00:00, map_index=-1, run_start_date=2025-03-11 10:29:37.249378+00:00, run_end_date=2025-03-11 10:30:03.772218+00:00, run_duration=26.52284, state=success, executor_state=success, try_number=1, max_tries=1, job_id=34, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:29:30.575382+00:00, queued_by_job_id=1, pid=186960[0m
[[34m2025-03-11T12:30:04.277+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 3 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.upload_beneficiary_to_s3 manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_claims_to_s3 manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.cleanup_part_d_data manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>[0m
[[34m2025-03-11T12:30:04.278+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:30:04.278+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T12:30:04.278+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T12:30:04.278+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.upload_beneficiary_to_s3 manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_claims_to_s3 manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.cleanup_part_d_data manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>[0m
[[34m2025-03-11T12:30:04.278+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task upload_beneficiary_to_s3 because previous state change time has not been saved[0m
[[34m2025-03-11T12:30:04.279+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task upload_claims_to_s3 because previous state change time has not been saved[0m
[[34m2025-03-11T12:30:04.279+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task cleanup_part_d_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:30:04.279+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_beneficiary_to_s3', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:30:04.279+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_beneficiary_to_s3', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:30:04.279+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_claims_to_s3', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T12:30:04.279+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_claims_to_s3', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:30:04.279+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='cleanup_part_d_data', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-03-11T12:30:04.279+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'cleanup_part_d_data', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:30:04.281+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_beneficiary_to_s3', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:30:04.886+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:30:04.969+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:30:05.002+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:30:05.002+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:30:05.084+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:30:05.169+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:29:08.321231+00:00/task_id=upload_beneficiary_to_s3 permission to 509
[[34m2025-03-11T12:30:05.444+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.upload_beneficiary_to_s3 manual__2025-03-11T10:29:08.321231+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:30:19.500+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_claims_to_s3', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:30:20.106+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:30:20.194+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:30:20.229+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:30:20.230+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:30:20.305+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:30:20.384+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:29:08.321231+00:00/task_id=upload_claims_to_s3 permission to 509
[[34m2025-03-11T12:30:20.646+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.upload_claims_to_s3 manual__2025-03-11T10:29:08.321231+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:34:10.964+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'cleanup_part_d_data', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:34:11.578+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:34:11.665+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:34:11.699+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:34:11.700+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:34:11.781+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:34:11.868+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:29:08.321231+00:00/task_id=cleanup_part_d_data permission to 509
[[34m2025-03-11T12:34:12.129+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.cleanup_part_d_data manual__2025-03-11T10:29:08.321231+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:34:12.587+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_beneficiary_to_s3', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:34:12.587+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_claims_to_s3', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:34:12.587+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='cleanup_part_d_data', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:34:12.590+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=cleanup_part_d_data, run_id=manual__2025-03-11T10:29:08.321231+00:00, map_index=-1, run_start_date=2025-03-11 10:34:12.162228+00:00, run_end_date=2025-03-11 10:34:12.253964+00:00, run_duration=0.091736, state=success, executor_state=success, try_number=1, max_tries=1, job_id=37, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-03-11 10:30:04.278455+00:00, queued_by_job_id=1, pid=193861[0m
[[34m2025-03-11T12:34:12.590+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=upload_beneficiary_to_s3, run_id=manual__2025-03-11T10:29:08.321231+00:00, map_index=-1, run_start_date=2025-03-11 10:30:05.475610+00:00, run_end_date=2025-03-11 10:30:19.034336+00:00, run_duration=13.558726, state=success, executor_state=success, try_number=1, max_tries=1, job_id=35, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:30:04.278455+00:00, queued_by_job_id=1, pid=187676[0m
[[34m2025-03-11T12:34:12.590+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=upload_claims_to_s3, run_id=manual__2025-03-11T10:29:08.321231+00:00, map_index=-1, run_start_date=2025-03-11 10:30:20.668540+00:00, run_end_date=2025-03-11 10:34:10.642119+00:00, run_duration=229.973579, state=success, executor_state=success, try_number=1, max_tries=1, job_id=36, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-03-11 10:30:04.278455+00:00, queued_by_job_id=1, pid=188130[0m
[[34m2025-03-11T12:34:12.601+0200[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=172849) last sent a heartbeat 248.35 seconds ago! Restarting it[0m
[[34m2025-03-11T12:34:12.605+0200[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 172849. PIDs of all processes in the group: [172849][0m
[[34m2025-03-11T12:34:12.606+0200[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 172849[0m
[[34m2025-03-11T12:34:12.698+0200[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=172849, status='terminated', exitcode=0, started='12:17:57') (172849) terminated with exit code 0[0m
[[34m2025-03-11T12:34:12.701+0200[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 193862[0m
[[34m2025-03-11T12:34:12.705+0200[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2025-03-11T12:34:12.718+0200] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-03-11T12:34:12.726+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T12:34:12.727+0200[0m] {[34mscheduler_job_runner.py:[0m1628} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2025-03-11T12:34:12.938+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 2 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.cleanup_beneficiary_data manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.cleanup_claims_data manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>[0m
[[34m2025-03-11T12:34:12.938+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T12:34:12.939+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T12:34:12.939+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.cleanup_beneficiary_data manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.cleanup_claims_data manual__2025-03-11T10:29:08.321231+00:00 [scheduled]>[0m
[[34m2025-03-11T12:34:12.939+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task cleanup_beneficiary_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:34:12.940+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task cleanup_claims_data because previous state change time has not been saved[0m
[[34m2025-03-11T12:34:12.940+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='cleanup_beneficiary_data', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-03-11T12:34:12.940+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'cleanup_beneficiary_data', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:34:12.940+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='cleanup_claims_data', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-03-11T12:34:12.940+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'cleanup_claims_data', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:34:12.942+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'cleanup_beneficiary_data', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:34:13.542+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:34:13.641+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:34:13.678+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:34:13.679+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:34:13.767+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:34:13.860+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:29:08.321231+00:00/task_id=cleanup_beneficiary_data permission to 509
[[34m2025-03-11T12:34:14.148+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.cleanup_beneficiary_data manual__2025-03-11T10:29:08.321231+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:34:14.619+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'cleanup_claims_data', 'manual__2025-03-11T10:29:08.321231+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T12:34:15.316+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T12:34:15.428+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:34:15.465+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T12:34:15.465+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T12:34:15.582+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T12:34:15.694+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T10:29:08.321231+00:00/task_id=cleanup_claims_data permission to 509
[[34m2025-03-11T12:34:16.020+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.cleanup_claims_data manual__2025-03-11T10:29:08.321231+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T12:34:16.699+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='cleanup_beneficiary_data', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:34:16.699+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='cleanup_claims_data', run_id='manual__2025-03-11T10:29:08.321231+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T12:34:16.702+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=cleanup_beneficiary_data, run_id=manual__2025-03-11T10:29:08.321231+00:00, map_index=-1, run_start_date=2025-03-11 10:34:14.184202+00:00, run_end_date=2025-03-11 10:34:14.276632+00:00, run_duration=0.09243, state=success, executor_state=success, try_number=1, max_tries=1, job_id=38, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-03-11 10:34:12.939360+00:00, queued_by_job_id=1, pid=193905[0m
[[34m2025-03-11T12:34:16.702+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=cleanup_claims_data, run_id=manual__2025-03-11T10:29:08.321231+00:00, map_index=-1, run_start_date=2025-03-11 10:34:16.056122+00:00, run_end_date=2025-03-11 10:34:16.310096+00:00, run_duration=0.253974, state=success, executor_state=success, try_number=1, max_tries=1, job_id=39, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-03-11 10:34:12.939360+00:00, queued_by_job_id=1, pid=193983[0m
[[34m2025-03-11T12:34:16.831+0200[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun medicare_data_extract_s3 @ 2025-03-11 10:29:08.321231+00:00: manual__2025-03-11T10:29:08.321231+00:00, state:running, queued_at: 2025-03-11 10:29:08.325075+00:00. externally triggered: True> successful[0m
[[34m2025-03-11T12:34:16.832+0200[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=medicare_data_extract_s3, execution_date=2025-03-11 10:29:08.321231+00:00, run_id=manual__2025-03-11T10:29:08.321231+00:00, run_start_date=2025-03-11 10:29:09.337146+00:00, run_end_date=2025-03-11 10:34:16.832120+00:00, run_duration=307.494974, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-03-11 10:29:08.321231+00:00, data_interval_end=2025-03-11 10:29:08.321231+00:00, dag_hash=64c40b6194349f709e4b7ae23d0d66c8[0m
[[34m2025-03-11T12:39:12.876+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T12:44:13.040+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T12:49:13.211+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T12:54:13.374+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T12:59:13.535+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T13:04:13.698+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T13:09:13.863+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T13:14:14.013+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T13:19:14.183+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T13:24:14.345+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T13:29:14.595+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T13:34:14.761+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T13:39:15.033+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T13:44:15.105+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T13:49:15.256+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T13:54:15.408+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T13:59:15.582+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T14:04:15.706+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T14:09:15.852+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T14:14:15.999+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T14:19:47.512+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T14:24:47.651+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T14:29:47.784+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T14:34:47.907+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T14:39:48.024+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T14:44:48.167+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T14:49:48.313+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T14:54:49.013+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T14:59:49.145+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T15:04:49.274+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T15:09:49.422+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T15:14:49.568+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T15:19:49.736+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T15:24:49.869+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T15:29:50.000+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T15:34:50.143+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T15:37:36.335+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>[0m
[[34m2025-03-11T15:37:36.335+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T15:37:36.335+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>[0m
[[34m2025-03-11T15:37:36.336+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task create_base_directory because previous state change time has not been saved[0m
[[34m2025-03-11T15:37:36.337+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1) to executor with priority 14 and queue default[0m
[[34m2025-03-11T15:37:36.337+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:37:36.339+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_base_directory', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:37:37.011+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T15:37:37.111+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:37:37.152+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T15:37:37.153+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:37:37.256+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T15:37:37.350+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T13:37:35.838716+00:00/task_id=create_base_directory permission to 509
[[34m2025-03-11T15:37:37.602+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_base_directory manual__2025-03-11T13:37:35.838716+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T15:37:38.049+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_base_directory', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T15:37:38.051+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_base_directory, run_id=manual__2025-03-11T13:37:35.838716+00:00, map_index=-1, run_start_date=2025-03-11 13:37:37.633288+00:00, run_end_date=2025-03-11 13:37:37.718105+00:00, run_duration=0.084817, state=success, executor_state=success, try_number=1, max_tries=1, job_id=40, pool=default_pool, queue=default, priority_weight=14, operator=BashOperator, queued_dttm=2025-03-11 13:37:36.335976+00:00, queued_by_job_id=1, pid=403864[0m
[[34m2025-03-11T15:37:38.189+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 1 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>[0m
[[34m2025-03-11T15:37:38.189+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T15:37:38.189+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>[0m
[[34m2025-03-11T15:37:38.190+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task create_data_subdirectories because previous state change time has not been saved[0m
[[34m2025-03-11T15:37:38.190+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_data_subdirectories', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1) to executor with priority 13 and queue default[0m
[[34m2025-03-11T15:37:38.190+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_data_subdirectories', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:37:38.192+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'create_data_subdirectories', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:37:38.777+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T15:37:38.861+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:37:38.896+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T15:37:38.896+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:37:38.985+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T15:37:39.079+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T13:37:35.838716+00:00/task_id=create_data_subdirectories permission to 509
[[34m2025-03-11T15:37:39.372+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.create_data_subdirectories manual__2025-03-11T13:37:35.838716+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T15:37:39.884+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='create_data_subdirectories', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T15:37:39.887+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=create_data_subdirectories, run_id=manual__2025-03-11T13:37:35.838716+00:00, map_index=-1, run_start_date=2025-03-11 13:37:39.406853+00:00, run_end_date=2025-03-11 13:37:39.497490+00:00, run_duration=0.090637, state=success, executor_state=success, try_number=1, max_tries=1, job_id=41, pool=default_pool, queue=default, priority_weight=13, operator=BashOperator, queued_dttm=2025-03-11 13:37:38.189802+00:00, queued_by_job_id=1, pid=403937[0m
[[34m2025-03-11T15:37:40.035+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 3 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>[0m
[[34m2025-03-11T15:37:40.035+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T15:37:40.035+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T15:37:40.035+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T15:37:40.035+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>[0m
[[34m2025-03-11T15:37:40.036+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_beneficiary_data because previous state change time has not been saved[0m
[[34m2025-03-11T15:37:40.036+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_claims_data because previous state change time has not been saved[0m
[[34m2025-03-11T15:37:40.036+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task download_part_d_data because previous state change time has not been saved[0m
[[34m2025-03-11T15:37:40.037+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_beneficiary_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-03-11T15:37:40.037+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_beneficiary_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:37:40.037+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_claims_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default[0m
[[34m2025-03-11T15:37:40.037+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_claims_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:37:40.037+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_part_d_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-03-11T15:37:40.037+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_part_d_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:37:40.038+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_beneficiary_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:37:40.612+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T15:37:40.709+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:37:40.748+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T15:37:40.749+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:37:40.840+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T15:37:40.933+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T13:37:35.838716+00:00/task_id=download_beneficiary_data permission to 509
[[34m2025-03-11T15:37:41.206+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_beneficiary_data manual__2025-03-11T13:37:35.838716+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T15:37:45.712+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_claims_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:37:46.262+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T15:37:46.345+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:37:46.377+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T15:37:46.377+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:37:46.455+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T15:37:46.537+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T13:37:35.838716+00:00/task_id=download_claims_data permission to 509
[[34m2025-03-11T15:37:46.781+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_claims_data manual__2025-03-11T13:37:35.838716+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T15:39:07.715+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'download_part_d_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:39:08.384+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T15:39:08.468+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:39:08.501+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T15:39:08.502+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:39:08.584+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T15:39:08.673+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T13:37:35.838716+00:00/task_id=download_part_d_data permission to 509
[[34m2025-03-11T15:39:08.922+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.download_part_d_data manual__2025-03-11T13:37:35.838716+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T15:39:18.292+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_beneficiary_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T15:39:18.292+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_claims_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T15:39:18.292+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='download_part_d_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T15:39:18.295+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_beneficiary_data, run_id=manual__2025-03-11T13:37:35.838716+00:00, map_index=-1, run_start_date=2025-03-11 13:37:41.239167+00:00, run_end_date=2025-03-11 13:37:45.374877+00:00, run_duration=4.13571, state=success, executor_state=success, try_number=1, max_tries=1, job_id=42, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2025-03-11 13:37:40.036192+00:00, queued_by_job_id=1, pid=403984[0m
[[34m2025-03-11T15:39:18.295+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_claims_data, run_id=manual__2025-03-11T13:37:35.838716+00:00, map_index=-1, run_start_date=2025-03-11 13:37:46.805232+00:00, run_end_date=2025-03-11 13:39:07.357264+00:00, run_duration=80.552032, state=success, executor_state=success, try_number=1, max_tries=1, job_id=43, pool=default_pool, queue=default, priority_weight=5, operator=PythonOperator, queued_dttm=2025-03-11 13:37:40.036192+00:00, queued_by_job_id=1, pid=404125[0m
[[34m2025-03-11T15:39:18.295+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=download_part_d_data, run_id=manual__2025-03-11T13:37:35.838716+00:00, map_index=-1, run_start_date=2025-03-11 13:39:08.953571+00:00, run_end_date=2025-03-11 13:39:17.961884+00:00, run_duration=9.008313, state=success, executor_state=success, try_number=1, max_tries=1, job_id=44, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2025-03-11 13:37:40.036192+00:00, queued_by_job_id=1, pid=406170[0m
[[34m2025-03-11T15:39:18.306+0200[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=193862) last sent a heartbeat 98.29 seconds ago! Restarting it[0m
[[34m2025-03-11T15:39:18.311+0200[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 193862. PIDs of all processes in the group: [193862][0m
[[34m2025-03-11T15:39:18.311+0200[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 193862[0m
[[34m2025-03-11T15:39:18.403+0200[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=193862, status='terminated', exitcode=0, started='12:34:12') (193862) terminated with exit code 0[0m
[[34m2025-03-11T15:39:18.407+0200[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 406419[0m
[[34m2025-03-11T15:39:18.411+0200[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2025-03-11T15:39:18.423+0200] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-03-11T15:39:18.654+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 3 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>[0m
[[34m2025-03-11T15:39:18.654+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T15:39:18.654+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T15:39:18.655+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T15:39:18.655+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>[0m
[[34m2025-03-11T15:39:18.655+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_beneficiary_data because previous state change time has not been saved[0m
[[34m2025-03-11T15:39:18.655+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task extract_claims_data because previous state change time has not been saved[0m
[[34m2025-03-11T15:39:18.655+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task upload_part_d_to_s3 because previous state change time has not been saved[0m
[[34m2025-03-11T15:39:18.656+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_beneficiary_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-03-11T15:39:18.656+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_beneficiary_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:39:18.656+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_claims_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default[0m
[[34m2025-03-11T15:39:18.656+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_claims_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:39:18.656+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_part_d_to_s3', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T15:39:18.656+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_part_d_to_s3', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:39:18.658+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_beneficiary_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:39:19.264+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T15:39:19.359+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:39:19.397+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T15:39:19.397+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:39:19.482+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T15:39:19.581+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T13:37:35.838716+00:00/task_id=extract_beneficiary_data permission to 509
[[34m2025-03-11T15:39:19.850+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.extract_beneficiary_data manual__2025-03-11T13:37:35.838716+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T15:39:20.451+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'extract_claims_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:39:21.058+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T15:39:21.151+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:39:21.187+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T15:39:21.187+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:39:21.274+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T15:39:21.366+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T13:37:35.838716+00:00/task_id=extract_claims_data permission to 509
[[34m2025-03-11T15:39:21.634+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.extract_claims_data manual__2025-03-11T13:37:35.838716+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T15:39:23.892+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_part_d_to_s3', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:39:24.483+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T15:39:24.578+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:39:24.615+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T15:39:24.616+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:39:24.701+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T15:39:24.801+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T13:37:35.838716+00:00/task_id=upload_part_d_to_s3 permission to 509
[[34m2025-03-11T15:39:25.082+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.upload_part_d_to_s3 manual__2025-03-11T13:37:35.838716+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T15:40:24.137+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_beneficiary_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T15:40:24.137+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='extract_claims_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T15:40:24.137+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_part_d_to_s3', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T15:40:24.141+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=extract_beneficiary_data, run_id=manual__2025-03-11T13:37:35.838716+00:00, map_index=-1, run_start_date=2025-03-11 13:39:19.884865+00:00, run_end_date=2025-03-11 13:39:20.077884+00:00, run_duration=0.193019, state=success, executor_state=success, try_number=1, max_tries=1, job_id=45, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2025-03-11 13:39:18.655351+00:00, queued_by_job_id=1, pid=406439[0m
[[34m2025-03-11T15:40:24.141+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=extract_claims_data, run_id=manual__2025-03-11T13:37:35.838716+00:00, map_index=-1, run_start_date=2025-03-11 13:39:21.663488+00:00, run_end_date=2025-03-11 13:39:23.500212+00:00, run_duration=1.836724, state=success, executor_state=success, try_number=1, max_tries=1, job_id=46, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2025-03-11 13:39:18.655351+00:00, queued_by_job_id=1, pid=406483[0m
[[34m2025-03-11T15:40:24.141+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=upload_part_d_to_s3, run_id=manual__2025-03-11T13:37:35.838716+00:00, map_index=-1, run_start_date=2025-03-11 13:39:25.112145+00:00, run_end_date=2025-03-11 13:40:23.712648+00:00, run_duration=58.600503, state=success, executor_state=success, try_number=1, max_tries=1, job_id=47, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 13:39:18.655351+00:00, queued_by_job_id=1, pid=406596[0m
[[34m2025-03-11T15:40:24.152+0200[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=406419) last sent a heartbeat 65.52 seconds ago! Restarting it[0m
[[34m2025-03-11T15:40:24.158+0200[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 406419. PIDs of all processes in the group: [406419][0m
[[34m2025-03-11T15:40:24.158+0200[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 406419[0m
[[34m2025-03-11T15:40:24.250+0200[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=406419, status='terminated', exitcode=0, started='15:39:17') (406419) terminated with exit code 0[0m
[[34m2025-03-11T15:40:24.254+0200[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 407897[0m
[[34m2025-03-11T15:40:24.258+0200[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2025-03-11T15:40:24.267+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T15:40:24.268+0200[0m] {[34mscheduler_job_runner.py:[0m1628} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[2025-03-11T15:40:24.271+0200] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-03-11T15:40:24.497+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 3 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.upload_beneficiary_to_s3 manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_claims_to_s3 manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.cleanup_part_d_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>[0m
[[34m2025-03-11T15:40:24.497+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T15:40:24.497+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T15:40:24.497+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T15:40:24.497+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.upload_beneficiary_to_s3 manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.upload_claims_to_s3 manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.cleanup_part_d_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>[0m
[[34m2025-03-11T15:40:24.498+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task upload_beneficiary_to_s3 because previous state change time has not been saved[0m
[[34m2025-03-11T15:40:24.498+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task upload_claims_to_s3 because previous state change time has not been saved[0m
[[34m2025-03-11T15:40:24.498+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task cleanup_part_d_data because previous state change time has not been saved[0m
[[34m2025-03-11T15:40:24.498+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_beneficiary_to_s3', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T15:40:24.498+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_beneficiary_to_s3', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:40:24.498+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_claims_to_s3', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-03-11T15:40:24.498+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_claims_to_s3', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:40:24.498+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='cleanup_part_d_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T15:40:24.498+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'cleanup_part_d_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:40:24.500+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_beneficiary_to_s3', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:40:25.080+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T15:40:25.166+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:40:25.202+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T15:40:25.202+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:40:25.276+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T15:40:25.362+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T13:37:35.838716+00:00/task_id=upload_beneficiary_to_s3 permission to 509
[[34m2025-03-11T15:40:25.677+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.upload_beneficiary_to_s3 manual__2025-03-11T13:37:35.838716+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T15:40:47.758+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'upload_claims_to_s3', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:40:48.323+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T15:40:48.406+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:40:48.436+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T15:40:48.437+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:40:48.515+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T15:40:48.607+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T13:37:35.838716+00:00/task_id=upload_claims_to_s3 permission to 509
[[34m2025-03-11T15:40:48.848+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.upload_claims_to_s3 manual__2025-03-11T13:37:35.838716+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T15:45:19.183+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'cleanup_part_d_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:45:19.719+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T15:45:19.798+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:45:19.828+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T15:45:19.829+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:45:19.904+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T15:45:19.990+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T13:37:35.838716+00:00/task_id=cleanup_part_d_data permission to 509
[[34m2025-03-11T15:45:20.237+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.cleanup_part_d_data manual__2025-03-11T13:37:35.838716+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T15:45:20.667+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_beneficiary_to_s3', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T15:45:20.667+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='upload_claims_to_s3', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T15:45:20.668+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='cleanup_part_d_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T15:45:20.670+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=cleanup_part_d_data, run_id=manual__2025-03-11T13:37:35.838716+00:00, map_index=-1, run_start_date=2025-03-11 13:45:20.260753+00:00, run_end_date=2025-03-11 13:45:20.338266+00:00, run_duration=0.077513, state=success, executor_state=success, try_number=1, max_tries=1, job_id=50, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-03-11 13:40:24.497761+00:00, queued_by_job_id=1, pid=414956[0m
[[34m2025-03-11T15:45:20.670+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=upload_beneficiary_to_s3, run_id=manual__2025-03-11T13:37:35.838716+00:00, map_index=-1, run_start_date=2025-03-11 13:40:25.701556+00:00, run_end_date=2025-03-11 13:40:47.428402+00:00, run_duration=21.726846, state=success, executor_state=success, try_number=1, max_tries=1, job_id=48, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 13:40:24.497761+00:00, queued_by_job_id=1, pid=407942[0m
[[34m2025-03-11T15:45:20.670+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=upload_claims_to_s3, run_id=manual__2025-03-11T13:37:35.838716+00:00, map_index=-1, run_start_date=2025-03-11 13:40:48.876998+00:00, run_end_date=2025-03-11 13:45:18.801789+00:00, run_duration=269.924791, state=success, executor_state=success, try_number=1, max_tries=1, job_id=49, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-03-11 13:40:24.497761+00:00, queued_by_job_id=1, pid=408435[0m
[[34m2025-03-11T15:45:20.681+0200[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=407897) last sent a heartbeat 296.20 seconds ago! Restarting it[0m
[[34m2025-03-11T15:45:20.686+0200[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 407897. PIDs of all processes in the group: [407897][0m
[[34m2025-03-11T15:45:20.686+0200[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 407897[0m
[[34m2025-03-11T15:45:20.778+0200[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=407897, status='terminated', exitcode=0, started='15:40:23') (407897) terminated with exit code 0[0m
[[34m2025-03-11T15:45:20.781+0200[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 414957[0m
[[34m2025-03-11T15:45:20.785+0200[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2025-03-11T15:45:20.796+0200] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-03-11T15:45:20.997+0200[0m] {[34mscheduler_job_runner.py:[0m413} INFO[0m - 3 tasks up for execution:
	<TaskInstance: medicare_data_extract_s3.cleanup_beneficiary_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.cleanup_claims_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.catalog_medicare_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>[0m
[[34m2025-03-11T15:45:20.997+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 0/16 running and queued tasks[0m
[[34m2025-03-11T15:45:20.997+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 1/16 running and queued tasks[0m
[[34m2025-03-11T15:45:20.997+0200[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG medicare_data_extract_s3 has 2/16 running and queued tasks[0m
[[34m2025-03-11T15:45:20.997+0200[0m] {[34mscheduler_job_runner.py:[0m592} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: medicare_data_extract_s3.cleanup_beneficiary_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.cleanup_claims_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>
	<TaskInstance: medicare_data_extract_s3.catalog_medicare_data manual__2025-03-11T13:37:35.838716+00:00 [scheduled]>[0m
[[34m2025-03-11T15:45:20.998+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task cleanup_beneficiary_data because previous state change time has not been saved[0m
[[34m2025-03-11T15:45:20.998+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task cleanup_claims_data because previous state change time has not been saved[0m
[[34m2025-03-11T15:45:20.998+0200[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task catalog_medicare_data because previous state change time has not been saved[0m
[[34m2025-03-11T15:45:20.999+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='cleanup_beneficiary_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T15:45:20.999+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'cleanup_beneficiary_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:45:20.999+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='cleanup_claims_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-03-11T15:45:20.999+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'cleanup_claims_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:45:20.999+0200[0m] {[34mscheduler_job_runner.py:[0m635} INFO[0m - Sending TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='catalog_medicare_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-03-11T15:45:20.999+0200[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'catalog_medicare_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:45:21.001+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'cleanup_beneficiary_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:45:21.524+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T15:45:21.639+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:45:21.699+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T15:45:21.700+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:45:21.790+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T15:45:21.866+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T13:37:35.838716+00:00/task_id=cleanup_beneficiary_data permission to 509
[[34m2025-03-11T15:45:22.110+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.cleanup_beneficiary_data manual__2025-03-11T13:37:35.838716+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T15:45:22.514+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'cleanup_claims_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:45:23.064+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T15:45:23.141+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:45:23.176+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T15:45:23.176+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:45:23.268+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T15:45:23.409+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T13:37:35.838716+00:00/task_id=cleanup_claims_data permission to 509
[[34m2025-03-11T15:45:23.748+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.cleanup_claims_data manual__2025-03-11T13:37:35.838716+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T15:45:24.286+0200[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'medicare_data_extract_s3', 'catalog_medicare_data', 'manual__2025-03-11T13:37:35.838716+00:00', '--local', '--subdir', 'DAGS_FOLDER/medicare_data_download_dag.py'][0m
[[34m2025-03-11T15:45:24.876+0200[0m] {[34mdagbag.py:[0m536} INFO[0m - Filling up the DagBag from /home/liran/Documents/medicare_data_project/airflow/dags/medicare_data_download_dag.py[0m
[[34m2025-03-11T15:45:24.968+0200[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:45:25.003+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/liran/Documents/medicare_data_project/medicare-env/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-03-11T15:45:25.003+0200[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-03-11T15:45:25.091+0200[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-03-11T15:45:25.189+0200[0m] {[34mexample_python_operator.py:[0m86} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
Changing /home/liran/Documents/medicare_data_project/airflow/logs/dag_id=medicare_data_extract_s3/run_id=manual__2025-03-11T13:37:35.838716+00:00/task_id=catalog_medicare_data permission to 509
[[34m2025-03-11T15:45:25.434+0200[0m] {[34mtask_command.py:[0m416} INFO[0m - Running <TaskInstance: medicare_data_extract_s3.catalog_medicare_data manual__2025-03-11T13:37:35.838716+00:00 [queued]> on host liran-Zenbook[0m
[[34m2025-03-11T15:47:41.789+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='cleanup_beneficiary_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T15:47:41.789+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='cleanup_claims_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T15:47:41.789+0200[0m] {[34mscheduler_job_runner.py:[0m685} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='medicare_data_extract_s3', task_id='catalog_medicare_data', run_id='manual__2025-03-11T13:37:35.838716+00:00', try_number=1, map_index=-1)[0m
[[34m2025-03-11T15:47:41.792+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=catalog_medicare_data, run_id=manual__2025-03-11T13:37:35.838716+00:00, map_index=-1, run_start_date=2025-03-11 13:45:25.466611+00:00, run_end_date=2025-03-11 13:47:41.464541+00:00, run_duration=135.99793, state=success, executor_state=success, try_number=1, max_tries=1, job_id=53, pool=default_pool, queue=default, priority_weight=1, operator=GlueCrawlerOperator, queued_dttm=2025-03-11 13:45:20.998095+00:00, queued_by_job_id=1, pid=415126[0m
[[34m2025-03-11T15:47:41.792+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=cleanup_beneficiary_data, run_id=manual__2025-03-11T13:37:35.838716+00:00, map_index=-1, run_start_date=2025-03-11 13:45:22.139340+00:00, run_end_date=2025-03-11 13:45:22.224003+00:00, run_duration=0.084663, state=success, executor_state=success, try_number=1, max_tries=1, job_id=51, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-03-11 13:45:20.998095+00:00, queued_by_job_id=1, pid=415025[0m
[[34m2025-03-11T15:47:41.792+0200[0m] {[34mscheduler_job_runner.py:[0m722} INFO[0m - TaskInstance Finished: dag_id=medicare_data_extract_s3, task_id=cleanup_claims_data, run_id=manual__2025-03-11T13:37:35.838716+00:00, map_index=-1, run_start_date=2025-03-11 13:45:23.775251+00:00, run_end_date=2025-03-11 13:45:23.943240+00:00, run_duration=0.167989, state=success, executor_state=success, try_number=1, max_tries=1, job_id=52, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-03-11 13:45:20.998095+00:00, queued_by_job_id=1, pid=415105[0m
[[34m2025-03-11T15:47:41.803+0200[0m] {[34mmanager.py:[0m302} ERROR[0m - DagFileProcessorManager (PID=414957) last sent a heartbeat 140.83 seconds ago! Restarting it[0m
[[34m2025-03-11T15:47:41.807+0200[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 414957. PIDs of all processes in the group: [414957][0m
[[34m2025-03-11T15:47:41.807+0200[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 414957[0m
[[34m2025-03-11T15:47:41.899+0200[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=414957, status='terminated', exitcode=0, started='15:45:19') (414957) terminated with exit code 0[0m
[[34m2025-03-11T15:47:41.904+0200[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 417964[0m
[[34m2025-03-11T15:47:41.908+0200[0m] {[34msettings.py:[0m61} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2025-03-11T15:47:41.918+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2025-03-11T15:47:41.919+0200] {manager.py:410} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-03-11T15:47:41.920+0200[0m] {[34mscheduler_job_runner.py:[0m1628} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2025-03-11T15:47:42.109+0200[0m] {[34mdagrun.py:[0m653} INFO[0m - Marking run <DagRun medicare_data_extract_s3 @ 2025-03-11 13:37:35.838716+00:00: manual__2025-03-11T13:37:35.838716+00:00, state:running, queued_at: 2025-03-11 13:37:35.847816+00:00. externally triggered: True> successful[0m
[[34m2025-03-11T15:47:42.109+0200[0m] {[34mdagrun.py:[0m704} INFO[0m - DagRun Finished: dag_id=medicare_data_extract_s3, execution_date=2025-03-11 13:37:35.838716+00:00, run_id=manual__2025-03-11T13:37:35.838716+00:00, run_start_date=2025-03-11 13:37:36.314243+00:00, run_end_date=2025-03-11 13:47:42.109550+00:00, run_duration=605.795307, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-03-11 13:37:35.838716+00:00, data_interval_end=2025-03-11 13:37:35.838716+00:00, dag_hash=e26c25dff9b329519f203a6c7783302a[0m
[[34m2025-03-11T15:52:42.082+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T15:57:42.261+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T16:02:42.417+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T16:07:42.578+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T16:12:42.728+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T16:17:42.747+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T16:22:42.921+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T16:27:43.295+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T16:32:43.456+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T16:37:43.607+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T16:42:43.643+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T16:47:43.813+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T16:52:43.989+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T16:57:44.160+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T17:02:44.321+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T17:11:09.963+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T17:16:10.093+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T17:21:10.224+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T17:26:10.362+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T17:31:10.493+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T17:36:10.628+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-03-11T17:41:10.760+0200[0m] {[34mscheduler_job_runner.py:[0m1605} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
