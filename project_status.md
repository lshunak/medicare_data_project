# Medicare Data Project Status

## Project Overview
- **Project Name**: Medicare Data Pipeline
- **Owner**: lshunak
- **Last Updated**: 2025-03-26 12:26:43

## Implementation Status   
- âœ… Airflow DAG setup and configuration
- âœ… Data extraction from source
- âœ… S3 upload functionality
- âœ… Local file cleanup after upload
- âœ… Parquet conversion
- âœ… AWS Glue cataloging
- ðŸ”„ Data loading to warehouse (In Progress)
- â¬œ Data transformation layer
- â¬œ Analytics and dashboard implementation
- â¬œ Containerization and deployment
- â¬œ Optional: Data streaming setup

## Next Steps
- [x] Implement data lake architecture
  - âœ… Configure S3 as landing zone
  - âœ… Convert data to Parquet format
  - âœ… Set up AWS Glue catalog for schema discovery
  - âœ… Set up partitioning by year
  - â¬œ Create Athena views for ad-hoc analysis
- ðŸ”„ Create data warehouse schema (Snowflake)
  - âœ… Set up Snowflake environment
  - ðŸ”„ Create load process from S3 to warehouse (In Progress)
  - â¬œ Set up transformation layer for analytics
- â¬œ Create dashboard for Medicare data insights
- â¬œ Schedule monthly data refresh process

## S3 Bucket Organization
- **Bucket Name**: lshunak-cms-bucket
- **Directory Structure**:
  - `raw/` - Original CSV files
    - `beneficiary/` - Beneficiary data CSV files
    - `claims/` - Claims data files 
    - `part_d/` - Part D prescription data
  - `processed/` - Parquet files
    - `beneficiary/` - Beneficiary data in Parquet format
    - `claims/` - Claims data in Parquet format
    - `part_d/` - Part D data in Parquet format
  - `scripts/` - ETL scripts
  - `temp/` - Temporary processing files
  - `spark-logs/` - Spark processing logs

## Glue Catalog Status
- **Database**: medicare_catalog
- **Tables**: 
  - `beneficiary` - Partitioned by year (2015-2025)
  - `claims` - Partitioned by year
  - `part_d` - Partitioned by year

## Snowflake Environment Status
- **Warehouse**: MEDICARE_DEV_WH (XSMALL, Auto-suspend 300s)
- **Databases**:
  - MEDICARE_BRONZE_DB
  - MEDICARE_SILVER_DB
  - MEDICARE_GOLD_DB 
- **Current Focus**: Implementing bronze layer loading using external tables
  
## Deployment Notes
- Running in local Airflow instance on Linux (Ubuntu)
- AWS connection ID set up as `aws_default` with proper permissions
- Local data temporarily stored in `~/Documents/medicare_data_project/data/raw/`
- Glue jobs configured with G.1X workers for optimal performance